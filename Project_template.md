## Задание 1. Анализ моделей и инфраструктуры

### Ключевые требования и ограничения

1. Конфиденциальность данных: часть данных содержит описание внутренних процессов, информацию о заказчиках (PDF-спецификации) и решениях, коммерческую тайну и т.д. Их утечка недопустима.

2. Качество ответов и скорость работы: должны быть достаточно высокими, чтобы ускорить работу сотрудников

3. Масштабируемость: решение должно адаптироваться под нужды компании, расти вместе с ней. Также возможны пути расширения - предложение чат-бота для клиентов компании

Учитывая конфиденциальность данных, использование публичных облачных API (OpenAI, YandexGPT) для обработки чувствительных материалов почти исключено (иначе придется провести большую работу по анонимизации чувствительной информации, причем при условии расширения базы знаний). Следовательно, фокус смещается на локальное или приватное облачное развёртывание.

### Сравнение LLM-моделей

Как было отмечено выше, конфиденциальность данных является ключевым ограничением, поэтому облачные решения являются вторичными для данной задачи.

| Критерий | Локальные Hugging Face | Облачные OpenAI / YandexGPT |
| - | - | - |
| Качество ответов | Постоянно улучшаются. Современные 70B+ модели на подходе к GPT-3.5, но до GPT-4-turbo ещё есть gap. Требуют тонкой настройки (fine-tuning) для специализированных задач. | Лучшее в индустрии. GPT-4-turbo выдаёт самые точные и связные ответы на сложные запросы "из коробки". |
| Скорость работы | Зависит от железа. На GPU (например, H100) скорость высокая (десятки токенов/с). На CPU может быть неприемлемо медленно для интерактивного чата. | Очень высокая и стабильная. Скорость предоставляется как услуга и не требует заботы о инфраструктуре. |
| Стоимость владения и использования | Высокие капитальные затраты (покупка серверов/GPU, либо использование какого-то облака для разворачивания RAG) | Нулевые CapEx, высокие OpEx. Плата за токен. При высоком объёме запросов месячный счёт может быть очень большим.|
| Удобство и простота развертывания | Сложно. Требует глубоких знаний в MLOps, настройке GPU, оркестрации контейнеров (Docker, Kubernetes). | Невероятно просто. Просто API-вызов. Не нужно думать об инфраструктуре, обновлениях моделей. |

Отдельно замечу, что для постоянного интенсивного использования локальное решение окупается. Облачное предсказуемо как операционный расход (про капитальные затраты и операционные расходы)

### Сравнение моделей эмбеддингов


| Критерий | Локальные Sentence-Transformers | Облачные OpenAI Embeddings |
| - | - | - |
| Скорость создания индекса | - | - |
| Качество поиска | - | - |
| Стоимость владения и использования | - | - |

### Сравнение векторных баз ChromaDB и FAISS

| Критерий | ChromaDB | FAISS |
| - | - | - |
| Скорость поиска и индексации | - | - |
| Сложность внедрения и поддержки | - | - |
| Удобство в работе | - | - |
| Стоимость владения (учет инфраструктуры) | - | - |

### Конфигурация сервера (CPU/GPU)


### Примеры конфигураций и выводы


## Задание 2. Подготовка базы знаний

Итоговая база знаний находится внутри knowledge_base

В качестве "Вселенной" была взята Вселенная Стального Алхимика. Сайт вики: https://fma.fandom.com/wiki/Main_Page

Краткая информация: «Стальной алхимик» (яп. 鋼の錬金術師 хаганэ но рэнкиндзюцуси, англ. Fullmetal Alchemist) — манга Хирому Аракавы. Манга ежемесячно издавалась с августа 2001 года по июнь 2010 года в журнале Shonen Gangan, принадлежащем компании Square Enix.

Для определения ключевых сущностей (которые могут потребовать замены в текстах) использовался код в скрипте [crucial_terms_definition.py](data_preparation/crucial_terms_definition.py). Он формирует файл terms_map_raw.json, который требует потом ручного анализа.

Для выгрузки данных со страниц вики использовался код [form_before_processing_base.py](data_preparation/form_before_processing_base.py). (часть страниц потом все равно была пост-обработана вручную.)

Сам процесс замены находится внутри файла [ __main__.py ](data_preparation/__main__.py)


## Задание 3. Создание векторного индекса базы знаний

Создание индекса находится внутри файла [__main__.py](index_creation/__main__.py).

По результатам его работы была создана коллекция CromaDB - knowledge_base на основе 30 документов, которые были подготовлены на предыдущем шаге.

Сам индекс ChromaDB находится внутри папки ./index_creation/vector_index/.

В качестве модели для создания эмбеддингов использовалась: sentence-transformers/all-MiniLM-L6-v2

Для разбиения текстов на чанки использовалась RecursiveCharacterTextSplitter. Размер чанка = 500, overlap = 50.

Размер эмбеддинга составил 384.

Общее время генерации индекса - около 18 секунд.

Количество чанков = 1597.


Чтобы протестировать работу базы знаний, можно запустить файл [test_work.py](index_creation/test_work.py). Этот файл запускает интерактивный режим работы, внутри которого можно задавать различные query для поиска.
Пример такой сессии: ![пример](index_creation/test_working.png)


## Задание 4. Реализация RAG-бота с техниками промптинга