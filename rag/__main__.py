import os
import numpy as np
from jinja2 import Environment, FileSystemLoader, Template
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer
import openai
from chromadb import PersistentClient
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import json
from pathlib import Path
import torch

MODEL_NAME = "openai/gpt-oss-120b"

class RAGPipeline:
    __templates_path__ = "./templates"
    __persisted_chroma_dir__ = "../index_creation/vector_index"
    __prompt_template_name__ = "prompt.j2"
    __question_template_name__ = "question.j2"
    
    def __init__(self, 
                 embedding_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                 vector_db_type: str = "chroma",  # –∏–ª–∏ "faiss" (–∫–∞–∫ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –±—É–¥—É—â–µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)
                 llm_provider: str = "openai"):  # –∏–ª–∏ "yandexgpt", "local"
        
        self.embedding_model = SentenceTransformer(embedding_model_name)
        self.vector_db_type = vector_db_type
        self.llm_provider = llm_provider
        
        if vector_db_type == "chroma":
            self.chroma_client = PersistentClient(path=self.__persisted_chroma_dir__)
            self.collection = self.chroma_client.get_or_create_collection(name="knowledge_base")
        elif vector_db_type == "faiss":
            raise NotImplementedError("This vector_db_type wasn't implemented yet")
        else:
            raise TypeError("unknown vector db technology")
        
        self.template_env = Environment(
            loader=FileSystemLoader(Path(self.__templates_path__)),
            autoescape=True,
            trim_blocks=True,
            lstrip_blocks=True
        )

        if llm_provider == "local":
            # –Ω—É–∂–µ–Ω —Ç–æ–∫–µ–Ω –≤–Ω—É—Ç—Ä–∏ os.environ["HF_TOKEN"]
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
            self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
            
            self.generator = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                torch_dtype="auto",
                device_map="auto",
            )
    
    def create_embeddings(self, texts: List[str]) -> np.ndarray:
        return self.embedding_model.encode(texts)
    
    def search_similar_chunks(self, query: str, k: int = 5) -> List[str]:
        """–ò—â–µ—Ç k –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö —á–∞–Ω–∫–æ–≤"""
        query_embedding = self.create_embeddings([query])
        
        if self.vector_db_type == "chroma":
            results = self.collection.query(
                query_embeddings=query_embedding.tolist(),
                n_results=k
            )
            return results['documents'][0]
    
    def create_prompt_and_question(self, query: str, context_chunks: List[str]) -> tuple[str, str]:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç —Å Few-shot –∏ CoT"""
        
        prompt_template = self.template_env.get_template(self.__prompt_template_name__)
        prompt = prompt_template.render(chunks=context_chunks, question=query)

        question_template = self.template_env.get_template(self.__question_template_name__)
        question = question_template.render(chunks=context_chunks, question=query)
        
        return prompt, question
    
    def call_llm(self, prompt: str, question: str) -> str:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ LLM"""
        
        if self.llm_provider == "openai":
            # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –≤–∞—à API –∫–ª—é—á: os.environ["OPENAI_API_KEY"] = "your-key"
            client = openai.OpenAI(
                api_key=os.environ["OPENAI_API_KEY"]
            )
            
            response = client.responses.create(
                model="gpt-3.5-turbo",
                instructions=prompt,
                input=question
            )
            return response.output_text
        
        elif self.llm_provider == "yandexgpt":
            raise NotImplementedError("This type wasn't implemented yet")
        
        elif self.llm_provider == "local":
            formatted_prompt = f"[INST] {prompt} {question} [/INST]"
            outputs = self.generator(formatted_prompt)
            return outputs[0]["generated_text"]
        
        return "LLM provider not implemented"
    
    def process_query(self, query: str, k: int = 5) -> str:
        """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞"""
        print(f"\n–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∑–∞–ø—Ä–æ—Å: {query}\n")
        print("=" * 80)
        
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        similar_chunks = self.search_similar_chunks(query, k)
        print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(similar_chunks)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤\n")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
        prompt, question = self.create_prompt_and_question(query, similar_chunks)
        print("üìù –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç:\n")
        print(prompt)
        print(question)
        print("=" * 80)
        
        # # –í—ã–∑–æ–≤ LLM
        print("‚≠ê –†–µ–∑—É–ª—å—Ç–∞—Ç:\n")
        response = self.call_llm(prompt, question)
        print(response)
        
        print("=" * 80)

def main():
    rag = RAGPipeline(vector_db_type="chroma", llm_provider="local")
    
    print("üîç –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã")
    print("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å (–∏–ª–∏ 'quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞):")
    
    while True:
        try:
            query = input("\n–í–æ–ø—Ä–æ—Å: ").strip()
            
            if query.lower() in ['quit', 'exit', 'q']:
                break
            
            if not query:
                continue
            # —Ä–∞–±–æ—Ç–∞ RAG –ø–∞–π–ø–ª–∞–π–Ω–∞
            rag.process_query(query)
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")

if __name__ == "__main__":
    main()